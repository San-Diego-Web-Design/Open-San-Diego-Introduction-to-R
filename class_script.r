###############################
# R Environment(s)
#
# We're in an R IDE. There are many different R IDEs and you should play 
# around to find one you like RStudio, JGE, REclipse, ... etc
#
# Quick note. some of the packages in this tutorial have issues with some of the 64bit R
# versions so please use a 32 bit version for this.
#
# Within the ide we've got a console window in which R code will execute. 
# You can type commands directly into the console. We also have one or 
# more script windows into which we can save R code for later. I recommend
# saving everything into script files so you can repeat it later. I also 
# recommend using those files to build a single script that will do a single 
# analysis. 
#
# This allows you to build an audit trail with which you can
#  - Reproduce your work
#  - Show others exactly how you came up with your results
#  - Allow you to easily repeat your analysis on a set of data in the same 
#    format
#
# In most r IDEs you can select some lines in a script and hit a keyboard 
# command (windows it's usually ctrl-r or ctrl-enter

###############################
# Basic R Syntax

# Basic r use is like a calculator
1 + 1

# Don't forget about the order of operations
1 + 1 * 3
(1 + 1) * 3

# We can assign as many variables as we like with <-
x <- 2
x
x + 1

# You can also use = for assignment but <- is a bit more common in practice
y = x + 1
y

# A lot of the commands in r are named for similar unix/linux shell commands like..

#list the variables
ls() 

#delete variables
rm(x,y)

#Variables have Data Types which are known as Modes in R

x <- 2
mode(x)

# note that it's dynamically typed. I don't declare that x is a number and I can 
# re-assign it to a character mode with no error
x <- "hello"
mode(x)

#and naturally it won't let me do math unless there are compatible Modes
x + 2

#logical mode (boolean true/false)
x = TRUE
mode(x)

#you can do explicit tests with

is.character(x)
is.logical(x)
is.character(x)


# There are four special constants, NULL, NA, Inf, and NaN.
# NULL is used to indicate an object is empty. We'll get to objects shortly.
c()

# NA is used for absent (?Not Available?) data values. 
z <- NA
z
is.na(z)

# Inf means infinity 
1 / 0

# NaN means not-a-number 
0 / 0


# Conditional Logic

x <- 3
y <- 5

# == is a conditional equality test
x == y
x

# Not the same as assignment. dont' make this typo. this is a decent reason
# to use <- for assignment. it's harder to make this mistake
x = y
x

# Here are some other logical operations
x != y
x > y
x <= y
x < y & x == 5
x < y & x == 4
(x < y & x == 4) | y == 4

# You might expect this to return TRUE but it doesn't. Use is.na 
z <- NA
z == NA

# NA


#conditional logic with if else 
if (x == y) { 
  y <- y - 1
  print("if!")
} else {
  print("else!")
}

#Looping

y <- 5
x <- 5

while(y > 0){
  y
  y <- y - 1
}

#doing a loop this way is ok, but in R frequently the actions you want to
#do repeatedly need to apply to every member or some subset of a data 
#structure and there are superior ways to do that, but first we have to
#learn about data structures and functions which we'll get to momentarily


###############################
# Working Directory

#the working directory is where R will store it's .RData file which 
#contains the current state of your R session. Variables, data, etc. 
#Plots are not stored. The working directory is also where all files
#generated by R or imported in are stored. Create a new working directory
#for each project, add your data files, notes, etc to the directory.

getwd()
# on my system the result is
# [1] "C:/Users/brian/Documents" 
# I really don't want to keep my work here. Lets change it.

# Notice that in the directory even though I'm on windows I used / instead of
# \. Like many languages character strings in R use \ as an escape character
# to add special characters so it's easier to just do paths with /
setwd("C:/Users/brian/Desktop/R Class")

# NOTE: you should replace this dir with a dir on your local system that you
# want to store your workspace in.

#files like .RData and .RHistory will show up in here and you should
#place the rest of the data files for your project here as well

#list files in the working directory
list.files()

###############################
# Installing and Loading Packages 
#
# R Packages bring in additional functionality. Most packages can be 
# automatically downloaded and installed from CRAN using install.packages

install.packages("XML")
install.packages("Deducer")
install.packages("maptools")

# You've got to load them for use too. Notice that loading/installing a 
# package will load/install packages it depends on as well. In this case 
# I can just load Deducer and it'll get ggplot2 and other required packages
# for me as well. install.packages does the same thing.

library(Deducer)
library(maptools)

# This command is special for maptools to enable some functions. you can 
# ignore it for now.
gclibPermit()


###############################
# Vectors
#
# All data in R is a vector. there are more complex structures built from 
# vectors, but this is the building block for data in r the c function 
# creates vectors.

odd <- c(1,3,5,7,9,11)
odd

# A shortcut for creating series
ten <- c(1:10)
ten
rm(ten)

# Vectors contain items of a single mode (data type). R will try and guess
# the type when you mix and match like so
mixed <- c(1,"hello")
mode(mixed)
rm(mixed)

# You can access individual items or groups of them with [] syntax
# Note: the first item is 1 not 0 in R

odd[1]
odd[3]
odd[2:4]

# Pass in a vector to select a list of items
odd[c(2,4)]


# You can do operations on vectors with scalars or other vectors. Much better
# than writing a loop to do something to a vector the result is another 
# vector
odd + 1

v2 <- c(1,2,3,4,5,6)

odd + v2

# Note that it took odd[1] + v2[1] and so forth. be careful with vectors of 
# mismatched size
v2 <- c(1,2)
v2 + odd

# Look what it did in the results. it alternated v1[1] + odd[x] 
# and v1[n] + odd[n]
# [1]  2  5  6  9 10 13 


# Lots of functions take vectors as input

length(odd)
mean(odd)
sd(odd)
summary(odd)


# Side Note: Getting Help on Functions and Packages

# Get help on the "mean" function
?mean

# Search for help with "mean" as the search. this is a regular expression
??mean

# Get help on a package
help(package="ggplot2")



# Vectors can have additional attributes to describe the contents and inform
# R how to work with the vector (metadata). These are just a list of key=value
# pairs. We don't have any just yet.

x <- c(1:100)

attributes(x)

# Vectors have dimensions. using matrix() array() and dim() manipulate the 
# dimensions of your vector

# So we have no dimensions defined right now. 
dim(x)
dim(x) <- c(10,10)
dim(x)
attributes(x)

x

# Now I can refer to individual items with row,column syntax
x[7,2]
x[7,]
x[,2]
median(x[,2])

# Another standard attribute is colnames
colnames(x) <- c("A","B","C","D","E","F","G","H","I","J")
x
x[,"E"]

# And I can make up extras of my own 
attr(x,'custom') = "Some random metadata"
attributes(x)
attr(x,'custom')

###############################
# More complex data structures
#
# Vectors are all well and good but usually you're going to be working
# something at least as complex as a table of data.

# Lists are similar to vectors but they can include items of varrying mode
x <- list(1,TRUE)
x

# So what's with the [[1]] and [[2]] items? You can see the list is actually made up of
# two seperate vectors in our case and we can refer to them with [[]]
# For lists, one generally uses [[ to select any single element, whereas 
# [ returns a list of the selected elements.

x[[2]]
x[[2]] <- c(TRUE,FALSE,FALSE,TRUE,FALSE)
x

# One simple thing lists are good for are keeping a lot of variables in one place. I
# often make one for temporary data
tmp <- list()

tmp <- list(bools = c(TRUE,FALSE,FALSE,TRUE,FALSE), oneHundred = c(1:100))
tmp
tmp[["bools"]]

# The $ syntax is a bit more convenient
tmp$bools

# Then I can dump all my temporary variables at once with
rm(tmp)


# Earlier when we talked about looping we said that often you don't need to use loop
# syntax when you want to apply operations to sets of data. We've got some functions
# That will let us loop through a data structure running a function on each item.
??apply

# Apply and other ??apply functions run a function on each member of a data structure.

x <- list(alpha = c(1:10), beta = c(100:200), delta = c(-32:3003))
x

#lapply is for lists. We'll run the mean function on each list item
lapply(x,mean)
lapply(x,quantile)

#There are a number of other functions here worth checking out under ??apply
rm(x)

###############################
# Loading Data from Files
#
# You're almost never going to be typing your data into r. You're going to want to load
# it from files, sql databases, from remote ftp sites and apis over the web. So lets 
# load a few things up

# Lets load up a file. we're going to use the annual california star report which 
# details standardized test scores in california public schools annually
# From: http://star.cde.ca.gov/star2010/ResearchFileList.asp?ps=true&lstTestYear=2010&lstTestType=C&lstCounty=37&lstDistrict=&lstSchool=&lstGroup=1&lstSubGroup=1

##### Exit here and go look at the data files and specifications on the web
# Extract all the files to your working directory inside a new sub-directory for 
# organization

# There are two files in our pack, both comma seperated values (csv) files. One has the
# actual data and one has some extra category information that can be used with the 
# main set. Lets load the main set up first

# Lets store the dir name for convenience
dataFileDir = "C:/Users/brian/Desktop/R Class/datasets/California Star - All Students/" 

# Note: this should be the directory you placed all your data files in. You can do
# ./ to start at the working directory

# With the read.csv function we can load a csv file. Other delimited files can be 
# loaded with read.delim()

# Side Note: Paste means "transform to string and concatinate"
star2010 <- read.csv(paste(dataFileDir,"ca2010_1_csv_v3.txt",sep=""))

# star2010 is now a "data frame" which is effectively a table with rows and columns.

# We can take a look at the data in a table now in the gui, but it's big so it's slow
# to load
View(star2010)

# Notice that it has column names already that we pulled in from the csv file
colnames(star2010)

# We can explore the data a bit using functions. Summary is a good way to do this 
# text-wise. Notice that it produces different results with a data frame vs a vector
summary(star2010)

#we can refer to a dframe column with $ easily
summary(star2010$Grade)

#we can view it's dimensions with dim
dim(star2010)

#each column of our dframe is itself a vector so we can use all the vector functions on columns easily
max(star2010$Grade)
min(star2010$Grade)
count(star2010$Grade)
range(star2010$Grade)

# The with statement will let us avoid the $ syntax when doing a longer calculation
with(star2010, mean((Students.Tested / 100) * Percent.Tested,na.rm=TRUE))

# We've not seen na.rm=TRUE before. this means ignore NA values in the column when 
# doing the calculation.


# One thing we see in the set is that country, district, etc are signified by numeric 
# codes instead of friendly names but we have them in the other data file "entities"
entities2010 <- read.csv((paste(dataFileDir,"ca2010entities_csv.txt",sep="")))
View(entities2010)

# Now we have two data frames but what we want is to join them together. The merge
# function can do this. A neat thing about merge is it will automatically find the
# matching column names and use those for merging. this is basically an automatic
# LEFT INNER JOIN if you know sql

star.all <- merge(star2010,entities2010)
colnames(star.all)

# Side Note: If you do like to use sql check out the sqldf package which will let you
# use sql join syntax to do similar things.

# A handy tip for viewing the set is to use the [] syntax to limit the rows and columns
# returned. Here's row 1 to 10
View(star.all[1:10, ])

# Or just a few columns of the top 10
star.all[1:10, c("Grade", "Percentage.Below.Basic")]
View(star.all[1:10, c("Grade", "Percentage.Below.Basic")])

#and we should remember to delete things we don't need anymore. especially big things like our old dataframes. 
rm(star2010,entities2010)


# Now lets add another year worth of data into the mix from http://star.cde.ca.gov/ 
# We'll follow the same procedure to load and merge the star and entities files to one 
# dframe

temp.star <- read.csv(paste(dataFileDir,"ca2009_1_csv_v3.txt",sep=""))
temp.entities <- read.csv(paste(dataFileDir,"ca2009entities_csv.txt",sep=""))
temp.combined <- merge(temp.star,temp.entities)

# And then we can add that new temp.combined item to our star.all frame with rbind
# Note that RBind is simply appending the rows from 2009 onto the rows from 2010 and it
# will expect there are the same numbers of columns.

star.all <- rbind(star.all,temp.combined)

# And lets get rid of these temp variables
rm(temp.star,temp.entities,temp.combined)

# Now we can see both 2009 and 2010 are in the same frame
range(star.all$Test.Year)

# Now that we've got the sets merged together we don't need the numeric codes used to
# do the join anymore. lets remove a few columns from the set. a simple way to do this
# is to take a subset of star.all and exclude the columns we don't want by name. 
# -c means everything but the items in this vector

star.all <- subset(star.all, select = -c(County.Code,District.Code,School.Code,Charter.Number))

##########################
# Factors

#A factor is a vector with a specific number of levels (discrete values) used for
# categorizing, sorting, grouping data as opposed to use for math operations. When we
# loaded our data.frame up from the file R took a guess at which columns were factors.
# In several cases it guessed wrong. 

# R decided that Mean.Scale.Score is a factor when it's just a numeric. We can use
# unclass to de-factor it
is.factor(star.all$Mean.Scale.Score)
star.all$Mean.Scale.Score <- unclass(star.all$Mean.Scale.Score)

# Test.Id and Type.Id on the other hand should be factors, but R thinks they are
# numeric values so we can change them with the factor() function
is.factor(star.all$Test.Id)
star.all$Test.Id <- factor(star.all$Test.Id)
star.all$Type.Id <- factor(star.all$Type.Id)

levels(star.all$Test.Id)
levels(star.all$Type.Id)

# Looking at the test.id and type.id you can see something interesting about our
# dataset - it's hierarchical. Type.Id for instance designates a level in the hierarchy
# (county > district > school ..) the data is aggregated to these levels so we need to
# be careful about our math.

# From http://star.cde.ca.gov/star2010/research_fixfileformat.asp
# Type ID
# 04 = State
# 05 = County
# 06 = District
# 07 = School
# 09 = Independent Charter School
# 10 = Dependent Charter School  

# When we do our analysis we'll naturally want to use this as a filter so we don't 
# do something dumb like double count items. When we do something like this:
sum(star.all$Students.with.Scores,na.rm=TRUE)

# We've actually counted every student+score combo 4 or more times because we've added
# together all different levels. every student is in Type.ID 04 (california) and again
# in a county, a district, a school, etc

# So first off it would be nice to have the readable names available in the set too. We
# don't have those set up in a file to load but we can create a new data frame by hand
# and merge it in pretty easily.

temp.frame <- data.frame(
	Type.Id = factor(c(4,5,6,7,9,10)),
	Type.Name = factor(c("State","County","District","School","Indp Charter","Dep Charter"))
)

star.all <- merge(star.all,temp.frame)
rm(temp.frame)

levels(star.all$Type.Name)

# To limit by type.id or by type.name we can use the subset function this time with
# it's data filtering parameter

star.byState <- subset(star.all, Type.Name == "State")
View(star.byState)
star.byState$Type.Name

# But again we can see there are many lines with the same numer of students on them. If
# you look you can see that Test.Id is the reason. There are a bunch of seperate tests
# being used so we'll want to split by that as well when we plot. From the
# documentation:

#07   CST English-Language Arts
#08	CST Mathematics
#09	CST Algebra I
#10	CST Integrated Math 1
#11	CST Geometry
#12	CST Integrated Math 2
#13	CST Algebra II
#14	CST Integrated Math 3
#15	CST Summative High School Mathematics
#18	CST World History
#19	CST U.S. History
#20	CST Biology
#21	CST Chemistry
#22	CST Earth Science
#23	CST Physics
#24	CST Integrated/Coordinated Science 1
#25	CST Integrated/Coordinated Science 2
#26	CST Integrated/Coordinated Science 3
#27	CST Integrated/Coordinated Science 4
#28	CST General Mathematics
#29	CST History - Social Science Grade 8
#30	CAPA English-Language Arts
#31	CAPA Mathematics
#32	CST Science - Grade 5, Grade 8, and Grade 10 Life Science
#38	STS Reading-Language Arts
#39	STS Mathematics
#43	CAPA Science
#44	CMA English-Language Arts
#45	CMA Mathematics
#46	CMA Science - Grade 5, Grade 8, and Grade 10 Life Science
#47	STS Algebra I
#48	STS Geometry
#49	CMA Algebra I

# So we'll again want to create a new factor column 

temp.frame <- data.frame(
	Test.Id = factor(c(7,8,9,10,11,12,13,14,15,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,38,39,43,44,45,46,47,48,49)),
	Test.Name = factor(c(
"CST English-Language Arts",
"CST Mathematics",
"CST Algebra I",
"CST Integrated Math 1",
"CST Geometry",
"CST Integrated Math 2",
"CST Algebra II",
"CST Integrated Math 3",
"CST Summative High School Mathematics",
"CST World History",
"CST U.S. History",
"CST Biology",
"CST Chemistry",
"CST Earth Science",
"CST Physics",
"CST Integrated/Coordinated Science 1",
"CST Integrated/Coordinated Science 2",
"CST Integrated/Coordinated Science 3",
"CST Integrated/Coordinated Science 4",
"CST General Mathematics",
"CST History - Social Science Grade 8",
"CAPA English-Language Arts",
"CAPA Mathematics",
"CST Science - Grade 5, Grade 8, and Grade 10 Life Science",
"STS Reading-Language Arts",
"STS Mathematics",
"CAPA Science",
"CMA English-Language Arts",
"CMA Mathematics",
"CMA Science - Grade 5, Grade 8, and Grade 10 Life Science",
"STS Algebra I",
"STS Geometry",
"CMA Algebra I"
	))
)
star.all <- merge(star.all,temp.frame)
rm(temp.frame)

levels(star.all$Test.Name)

# Since these specification are in html we could have actually used R to scrape it off
# the web and load it into a dataset using The xml package's readHTMLTable function
# something like this:
library(XML)
tables <- readHTMLTable("./datasets/Fixed-Length ASCII Files Record Definitions - 2010.htm")
#tables <- readHTMLTable("http://star.cde.ca.gov/star2010/research_fixfileformat.asp")
tables
#and then we could merge from tables instead.


# If we wanted to look at a specific test like CST Algebra I by District we could use
# a compound subset

AlgIbyDistrict <- subset(star.all, Type.Id == 6 & Test.Id == 9)


# Another interesting bit is that since most of the data is aggregated into % values
# We have to be careful about what math we do on it. For instance even if only done at
# one level this isn't very useful.
median(AlgIbyDistrict$Percent.Tested, na.rm=TRUE)

# You can see how this degrades the value of the data set for us. It's much better to
# have a the data at a fine granularity without all the aggregations done for us ahead
# of time so we could decide how we want to aggregate and analyze. The state of
# California in this case has a pretty good reason for doing this; They want to
# protect the anonymity of students so they only provide the data at-aggregate. This
# way it's difficult to trace back an individual score to a specific person. 


###############################
# Plotting with ggplot2 http://had.co.nz/ggplot2/

# So now that we've got all our data together we can start to explore it visually. R
# comes with ok built in plotting, but there are many add on packages to do different
# types of plotting. Ggplot2 is one of the most popular and flexible, but it can be a
# challange to understand all the syntax and components.


# Components of a plot are Data, Geometric Objects (lines, marks, etc. known as geom), 
# statistical transformations, scales, coordinates, position adjustment, etc.
# A set of these can be combined into a layer and you can put many layers into one 
# plot

# We're going to use an additional package, Deducer, which provides a little gui
# for dealing with this stuff so we'll open it up to look at the components of a chart.
#deducer('Plot builder')

# There are two key functions ggplot() which is the full plot syntax but complex and 
# qplot() which is a quick plotting function. We're going to focus on qplot for now
# which allows for a bit more quick experimentation. qplot is basically just assuming
# a lot of parameters for you and running ggplot

# p1 will be our set for plotting for now.
p1 <- subset(star.all, Percent.Tested != 0 & !is.na(Percent.Tested) & Mean.Scale.Score != 0 
  & !is.na(Mean.Scale.Score) & Type.Name == "District")

colnames(p1)

# For a single measure qplot assumes a histogram
qplot(Mean.Scale.Score, data=p1)

#applying a different theme
last_plot() + theme_bw()

#editing the plot title
last_plot() + theme_bw() + opts(plot.title = theme_text(size = 20))

last_plot() + opts(panel.background = 
  theme_rect(linetype = "dotted"))
  
# The key point here is you can re-plot with the last_plot() otion and use + opts to apply
# various options

# We can adjust the bucket width with binwidth
qplot(Mean.Scale.Score, data=p1, binwidth=50)

# qplot assumes scatter plot for two vars
qplot(Mean.Scale.Score,Test.Name, data=p1)

# Lets reduce the alpha value (transperency) of the dots so we can better see where 
# there are lots of overlapping dots. Note the use of I() which forces a fixed value
# transperency value rather than varying alpha by the data
qplot(Mean.Scale.Score,Test.Name, data=p1, alpha=I(.2))

#now lets set a unique color for each year
qplot(Mean.Scale.Score,Test.Name, data=p1, alpha=I(.2), color=Test.Year)

#maybe it would be better to see a seperate plot for each grade. The ~ seperates row
# facets from type facets. The . is a placeholder when there's only one facet.
qplot(Mean.Scale.Score,Test.Name, data=p1, alpha=I(.2), color=Test.Year, facets=Grade~.)

#lets facet by both year and grade and we can drop color
qplot(Mean.Scale.Score,Test.Name, data=p1, alpha=I(.2), facets=Grade~Test.Year)

#its getting a bit hard to read and we're also breaking the rules of mean scaled score comparisons. 
#really we should only compare one grade at a time, one subject area at a time. so lets switch it up
#and facet by grade and subject area (test type) but use year and mean scale score to plot the points
qplot(Mean.Scale.Score,Test.Year, data=p1, facets=Grade~Test.Type)

# We could continue on to try and make this look better with visual parameters or use deducer to
# do it in a drag and drop style. The point is the plotting, while not immediatly attractive is
# extremly flexible in that you can add an unlimited number of plots together using different 
# markers, geometry options, and soforth


